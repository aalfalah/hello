
The emergence of Large Language Models (LLMs) has marked a fundamental shift in the landscape of Data & AI products, transforming them from narrowly focused scoring engines that produced structured, task-specific outputs (like scores, classifications, and recommendations) to  generative engines capable of producing unstructured, context-rich outputs. 

Traditional data products relied on fixed schemas and external preprocessing to handle context, delivering deterministic and predictable results. In contrast, LLMs dynamically incorporate context within their processing through mechanisms like attention, enabling them to generate free-form text, insights, and narratives that adapt to nuanced inputs. 

Sure, this shift has expanded the potential applications of AI, but also introduced new challenges in evaluation, monitoring, and ethical considerations. 

By embedding context as a core feature, LLMs have redefined what it means to build and interact with AI-driven systems, unlocking unprecedented flexibility and depth in user experiences.

I had the opportunity to explore these changes during an interactive session, in December 2024,   with Students from [University of Greenwich, UK](https://www.gre.ac.uk/) 

![[Evolution_of_Data_Products.pdf]]
